{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40beb46d-95af-4b60-8d64-5fd5c4172ad1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### THIS CODE WORKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4e1898f5-3562-4571-b2e2-1bea002e75de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped and saved match 13e27d90:\n",
      "  - 13e27d90_summary.csv\n",
      "  - 13e27d90_goalkeepers.csv\n",
      "  - 13e27d90_shots.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_and_save_full_match(url):\n",
    "    \"\"\"Scrapes outfield stats, goalkeeper stats, and shots data for a match and saves CSVs automatically.\"\"\"\n",
    "\n",
    "    # Extract game_id from URL\n",
    "    game_id = url.split(\"/\")[5]\n",
    "\n",
    "    # Fetch page\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Get match date\n",
    "    date_element = soup.select_one(\".venuetime\")\n",
    "    match_date = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "    # Get full team names using 'title' attribute fallback\n",
    "    scorebox = soup.select_one('.scorebox')\n",
    "    teams = []\n",
    "    if scorebox:\n",
    "        team_divs = scorebox.find_all('div', recursive=False)[:2]  # Only left/right team blocks\n",
    "        for div in team_divs:\n",
    "            team_tag = div.find('a')\n",
    "            if team_tag:\n",
    "                team_name = team_tag.get('title') or team_tag.text\n",
    "                teams.append(team_name.strip())\n",
    "\n",
    "    if len(teams) != 2:\n",
    "        raise Exception(f\"Couldn't find two team names. Teams found: {teams}\")\n",
    "\n",
    "    home_team, away_team = teams\n",
    "\n",
    "    # Get all tables\n",
    "    all_tables = soup.find_all(\"table\")\n",
    "    summary_tables = [t for t in all_tables if \"summary\" in t.get(\"id\", \"\") and \"keeper\" not in t.get(\"id\", \"\")]\n",
    "    keeper_tables = [t for t in all_tables if \"keeper\" in t.get(\"id\", \"\")]\n",
    "    shots_tables = [t for t in all_tables if \"shots\" in t.get(\"id\", \"\")]\n",
    "\n",
    "    outfield_rows = []\n",
    "    goalkeeper_rows = []\n",
    "    shots_rows = []\n",
    "\n",
    "    # Outfield players (2 tables)\n",
    "    for idx, table in enumerate(summary_tables):\n",
    "        team = home_team if idx == 0 else away_team\n",
    "        opponent = away_team if idx == 0 else home_team\n",
    "        home_away = \"Home\" if idx == 0 else \"Away\"\n",
    "\n",
    "        header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "        headers = [th.text.strip() for th in header_row.find_all(\"th\")]\n",
    "\n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            if row.get('class') and \"thead\" in row.get('class'):\n",
    "                continue\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                player_data = dict(zip(headers, cells))\n",
    "\n",
    "                # ✨ Clean player name and create player_id\n",
    "                player_name = player_data.get(\"Player\", \"\").strip()\n",
    "                jersey = player_data.get(\"#\", \"\").strip()\n",
    "                player_data[\"Player\"] = player_name\n",
    "                player_data[\"#\"] = jersey\n",
    "                player_data[\"player_id\"] = f\"{jersey}{player_name}\"\n",
    "\n",
    "                outfield_rows.append(player_data)\n",
    "\n",
    "    # Goalkeepers (2 tables)\n",
    "    for idx, table in enumerate(keeper_tables):\n",
    "        team = home_team if idx == 0 else away_team\n",
    "        opponent = away_team if idx == 0 else home_team\n",
    "        home_away = \"Home\" if idx == 0 else \"Away\"\n",
    "\n",
    "        header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "        headers = [th.text.strip() for th in header_row.find_all(\"th\")]\n",
    "\n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            if row.get('class') and \"thead\" in row.get('class'):\n",
    "                continue\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                player_data = dict(zip(headers, cells))\n",
    "\n",
    "                goalkeeper_rows.append(player_data)\n",
    "\n",
    "    # Shots (1 table shared)\n",
    "    for table in shots_tables:\n",
    "        header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "        headers = [th.text.strip() for th in header_row.find_all(\"th\")]\n",
    "\n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                shot_data = dict(zip(headers, cells))\n",
    "                shot_data.update({\n",
    "                    \"game_id\": game_id,\n",
    "                    \"match_date\": match_date\n",
    "                })\n",
    "                shots_rows.append(shot_data)\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    df_outfield = pd.DataFrame(outfield_rows)\n",
    "    df_goalkeepers = pd.DataFrame(goalkeeper_rows)\n",
    "    df_shots = pd.DataFrame(shots_rows)\n",
    "\n",
    "    # Save to CSVs\n",
    "    df_outfield.to_csv(f\"TEST{game_id}_summary.csv\", index=False)\n",
    "    df_goalkeepers.to_csv(f\"TEST{game_id}_goalkeepers.csv\", index=False)\n",
    "    df_shots.to_csv(f\"TEST{game_id}_shots.csv\", index=False)\n",
    "\n",
    "    print(f\"✅ Scraped and saved match {game_id}:\")\n",
    "    print(f\"  - {game_id}_summary.csv\")\n",
    "    print(f\"  - {game_id}_goalkeepers.csv\")\n",
    "    print(f\"  - {game_id}_shots.csv\")\n",
    "\n",
    "\n",
    "scrape_and_save_full_match(\"https://fbref.com/en/matches/13e27d90/Portland-Thorns-FC-Orlando-Pride-May-3-2025-NWSL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e5b44-e569-4c43-80be-9567491c868b",
   "metadata": {},
   "source": [
    "### PULL AND CLEAN SUMMARY AND GOALKEEPER TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d58a15fe-4c3e-4c3a-84bb-c3c3a121cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped and saved match 7958f078:\n",
      "  - 7958f078_summary.csv\n",
      "  - 7958f078_goalkeepers.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_and_save_full_match(url):\n",
    "    \"\"\"Scrapes outfield stats, goalkeeper stats, and shots data for a match and saves CSVs automatically.\"\"\"\n",
    "\n",
    "    # Extract game_id from URL\n",
    "    game_id = url.split(\"/\")[5]\n",
    "\n",
    "    # Fetch page\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Get match date\n",
    "    date_element = soup.select_one(\".venuetime\")\n",
    "    match_date = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "    # Get full team names using 'title' attribute fallback\n",
    "    scorebox = soup.select_one('.scorebox')\n",
    "    teams = []\n",
    "    if scorebox:\n",
    "        team_divs = scorebox.find_all('div', recursive=False)[:2]\n",
    "        for div in team_divs:\n",
    "            team_tag = div.find('a')\n",
    "            if team_tag:\n",
    "                team_name = team_tag.get('title') or team_tag.text\n",
    "                teams.append(team_name.strip())\n",
    "\n",
    "    if len(teams) != 2:\n",
    "        raise Exception(f\"Couldn't find two team names. Teams found: {teams}\")\n",
    "\n",
    "    home_team, away_team = teams\n",
    "\n",
    "    # Get all tables\n",
    "    all_tables = soup.find_all(\"table\")\n",
    "    summary_tables = [t for t in all_tables if \"summary\" in t.get(\"id\", \"\") and \"keeper\" not in t.get(\"id\", \"\")]\n",
    "    keeper_tables = [t for t in all_tables if \"keeper\" in t.get(\"id\", \"\")]\n",
    "    shots_tables = [t for t in all_tables if \"shots\" in t.get(\"id\", \"\")]\n",
    "\n",
    "    outfield_rows = []\n",
    "    goalkeeper_rows = []\n",
    "    shots_rows = []\n",
    "\n",
    "    # OUTFIELD PLAYERS\n",
    "    for idx, table in enumerate(summary_tables):\n",
    "        team = home_team if idx == 0 else away_team\n",
    "        opponent = away_team if idx == 0 else home_team\n",
    "        home_away = \"Home\" if idx == 0 else \"Away\"\n",
    "\n",
    "        header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "        headers = [th.text.strip() for th in header_row.find_all(\"th\")]\n",
    "\n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            if row.get('class') and \"thead\" in row.get('class'):\n",
    "                continue\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                player_data = dict(zip(headers, cells))\n",
    "\n",
    "                player_name = player_data.get(\"Player\", \"\").strip()\n",
    "                jersey = player_data.get(\"#\", \"\").strip()\n",
    "                clean_name = player_name.replace(\" \", \"\")\n",
    "                player_id = f\"{jersey}{clean_name}{game_id}\"\n",
    "\n",
    "                player_data.update({\n",
    "                    \"game_id\": game_id,\n",
    "                    \"player_id\": player_id\n",
    "                })\n",
    "\n",
    "                outfield_rows.append(player_data)\n",
    "\n",
    "    df_outfield = pd.DataFrame(outfield_rows)\n",
    "\n",
    "    # GOALKEEPERS\n",
    "    for idx, table in enumerate(keeper_tables):\n",
    "        team = home_team if idx == 0 else away_team\n",
    "        opponent = away_team if idx == 0 else home_team\n",
    "        home_away = \"Home\" if idx == 0 else \"Away\"\n",
    "    \n",
    "        header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "        headers = [th.text.strip() for th in header_row.find_all(\"th\")]\n",
    "    \n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            if row.get('class') and \"thead\" in row.get('class'):\n",
    "                continue\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                player_data = dict(zip(headers, cells))\n",
    "    \n",
    "                player_name = player_data.get(\"Player\", \"\").strip()\n",
    "                jersey = player_data.get(\"#\", \"\").strip()\n",
    "                clean_name = player_name.replace(\" \", \"\")\n",
    "                player_id = f\"{jersey}{clean_name}\"\n",
    "    \n",
    "                # Fallback if jersey is missing — try to match player name in df_outfield\n",
    "                if not jersey and player_name in df_outfield[\"Player\"].values:\n",
    "                    matched = df_outfield[df_outfield[\"Player\"] == player_name]\n",
    "                    if not matched.empty:\n",
    "                        jersey = matched.iloc[0][\"#\"]\n",
    "                        player_id = matched.iloc[0][\"player_id\"]\n",
    "    \n",
    "                player_data.update({\n",
    "                    \"#\": jersey,\n",
    "                    \"game_id\": game_id,\n",
    "                    \"player_id\": player_id\n",
    "                })\n",
    "    \n",
    "                goalkeeper_rows.append(player_data)\n",
    "\n",
    "    # Save CSVs\n",
    "    df_goalkeepers = pd.DataFrame(goalkeeper_rows)\n",
    "\n",
    "    df_outfield.to_csv(f\"{game_id}_summary.csv\", index=False)\n",
    "    df_goalkeepers.to_csv(f\"{game_id}_goalkeepers.csv\", index=False)\n",
    "\n",
    "    print(f\"✅ Scraped and saved match {game_id}:\")\n",
    "    print(f\"  - {game_id}_summary.csv\")\n",
    "    print(f\"  - {game_id}_goalkeepers.csv\")\n",
    "\n",
    "# Example use\n",
    "scrape_and_save_full_match(\"https://fbref.com/en/matches/7958f078/Portland-Thorns-FC-Angel-City-FC-March-21-2025-NWSL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5cb35-43d9-4384-8d95-55e13a0ee7eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GAME SCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99d8d2e5-802e-4415-9f2f-f5f8e010e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "def scrape_and_save_full_match_all_tables(url):\n",
    "    \"\"\"Scrapes full outfield player stats (all tables), goalkeepers, and shots, and saves 3 CSVs.\"\"\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    game_id = url.split(\"/\")[5]\n",
    "\n",
    "    # Match date\n",
    "    date_element = soup.select_one(\".venuetime\")\n",
    "    match_date = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "    # Team names\n",
    "    scorebox = soup.select_one('.scorebox')\n",
    "    teams = []\n",
    "    if scorebox:\n",
    "        team_divs = scorebox.find_all('div', recursive=False)[:2]\n",
    "        for div in team_divs:\n",
    "            team_tag = div.find('a')\n",
    "            if team_tag:\n",
    "                team_name = team_tag.get('title') or team_tag.text\n",
    "                teams.append(team_name.strip())\n",
    "    if len(teams) != 2:\n",
    "        raise Exception(\"Could not find two team names.\")\n",
    "    home_team, away_team = teams\n",
    "\n",
    "    # ---- OUTFIELD PLAYER STATS (All Tables) ---- #\n",
    "    table_types = {\n",
    "        \"summary\": \"Summary\",\n",
    "        \"passing\": \"Passing\",\n",
    "        \"passing_types\": \"Pass Types\",\n",
    "        \"defense\": \"Defensive\",\n",
    "        \"possession\": \"Possession\",\n",
    "        \"misc\": \"Miscellaneous\"\n",
    "    }\n",
    "\n",
    "    all_dfs = []\n",
    "    join_cols = [\"Player\", \"match_date\", \"game_id\", \"team\", \"opponent\", \"home_away\"]\n",
    "\n",
    "    for key in table_types:\n",
    "        tables = [t for t in soup.find_all(\"table\") if key in t.get(\"id\", \"\") and \"keeper\" not in t.get(\"id\", \"\")]\n",
    "\n",
    "        for idx, table in enumerate(tables):\n",
    "            team = home_team if idx == 0 else away_team\n",
    "            opponent = away_team if idx == 0 else home_team\n",
    "            home_away = \"Home\" if idx == 0 else \"Away\"\n",
    "\n",
    "            header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "            headers = [th.text.strip() for th in header_row.find_all(\"th\")]\n",
    "\n",
    "            rows = []\n",
    "            for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "                # Skip header rows inside body (multi-level headers)\n",
    "                if row.get('class') and \"thead\" in row.get('class'):\n",
    "                    continue\n",
    "                # Skip empty rows\n",
    "                if len(row.find_all([\"th\", \"td\"])) == 0:\n",
    "                    continue\n",
    "                cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "                if len(cells) != len(headers):\n",
    "                    continue\n",
    "                player_data = dict(zip(headers, cells))\n",
    "                # Skip team summary rows\n",
    "                if player_data.get(\"Player\", \"\").lower() in [\"team totals\", \"\"]:\n",
    "                    continue\n",
    "                player_data.update({\n",
    "                    \"game_id\": game_id,\n",
    "                    \"match_date\": match_date,\n",
    "                    \"team\": team,\n",
    "                    \"opponent\": opponent,\n",
    "                    \"home_away\": home_away\n",
    "                })\n",
    "                rows.append(player_data)\n",
    "\n",
    "            if not rows:\n",
    "                continue  # If no valid rows, skip\n",
    "\n",
    "            df = pd.DataFrame(rows)\n",
    "\n",
    "            # Rename all stat columns (not join columns)\n",
    "            df = df.rename(columns={col: f\"{key}_{col}\" if col not in join_cols else col for col in df.columns})\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        # Merge safely with outer join\n",
    "        df_outfield = reduce(lambda left, right: pd.merge(left, right, on=join_cols, how=\"outer\"), all_dfs)\n",
    "        df_outfield.to_csv(f\"{game_id}_outfield_players.csv\", index=False)\n",
    "        print(f\"✅ Saved {game_id}_outfield_players.csv\")\n",
    "    else:\n",
    "        print(f\"⚠️ No outfield player data found for {game_id}\")\n",
    "        df_outfield = pd.DataFrame()\n",
    "\n",
    "    # ---- GOALKEEPER TABLES ---- #\n",
    "    keeper_tables = [t for t in soup.find_all(\"table\") if \"keeper\" in t.get(\"id\", \"\")]\n",
    "    goalkeeper_rows = []\n",
    "\n",
    "    for idx, table in enumerate(keeper_tables):\n",
    "        team = home_team if idx == 0 else away_team\n",
    "        opponent = away_team if idx == 0 else home_team\n",
    "        home_away = \"Home\" if idx == 0 else \"Away\"\n",
    "\n",
    "        header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "        headers = [th.text.strip() for th in header_row.find_all(\"th\")]\n",
    "\n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            if row.get('class') and \"thead\" in row.get('class'):\n",
    "                continue\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                player_data = dict(zip(headers, cells))\n",
    "                player_data.update({\n",
    "                    \"game_id\": game_id,\n",
    "                    \"match_date\": match_date,\n",
    "                    \"team\": team,\n",
    "                    \"opponent\": opponent,\n",
    "                    \"home_away\": home_away\n",
    "                })\n",
    "                goalkeeper_rows.append(player_data)\n",
    "\n",
    "    df_goalkeepers = pd.DataFrame(goalkeeper_rows)\n",
    "    df_goalkeepers.to_csv(f\"{game_id}_goalkeepers.csv\", index=False)\n",
    "    print(f\"✅ Saved {game_id}_goalkeepers.csv\")\n",
    "\n",
    "    # ---- SHOTS TABLE ---- #\n",
    "    shot_tables = [t for t in soup.find_all(\"table\") if \"shots\" in t.get(\"id\", \"\")]\n",
    "    shots_rows = []\n",
    "\n",
    "    for table in shot_tables:\n",
    "        header_row = table.find(\"thead\").find_all(\"tr\")[-1]\n",
    "        headers = [th.text.strip() for th in header_row.find_all(\"th\")]\n",
    "\n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                shot_data = dict(zip(headers, cells))\n",
    "                shot_data.update({\n",
    "                    \"game_id\": game_id,\n",
    "                    \"match_date\": match_date\n",
    "                })\n",
    "                shots_rows.append(shot_data)\n",
    "\n",
    "    df_shots = pd.DataFrame(shots_rows)\n",
    "    df_shots.to_csv(f\"{game_id}_shots.csv\", index=False)\n",
    "    print(f\"✅ Saved {game_id}_shots.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8024f10-4f79-4cea-8ad3-ee860414de95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error scraping https://fbref.com/en/matches/7239a666/Kansas-City-Current-Portland-Thorns-FC-March-15-2025-NWSL: Passing 'suffixes' which cause duplicate columns {'passing_Age_x', 'passing_Nation_x', 'passing_#_x', 'passing_Att_x', 'passing_Cmp_x', 'passing_Min_x', 'passing_Pos_x'} is not allowed.\n",
      "❌ Error scraping https://fbref.com/en/matches/7958f078/Portland-Thorns-FC-Angel-City-FC-March-21-2025-NWSL: Passing 'suffixes' which cause duplicate columns {'passing_Age_x', 'passing_Nation_x', 'passing_#_x', 'passing_Att_x', 'passing_Cmp_x', 'passing_Min_x', 'passing_Pos_x'} is not allowed.\n",
      "❌ Error scraping https://fbref.com/en/matches/475a847a/Portland-Thorns-FC-North-Carolina-Courage-March-29-2025-NWSL: Passing 'suffixes' which cause duplicate columns {'passing_Age_x', 'passing_Nation_x', 'passing_#_x', 'passing_Att_x', 'passing_Cmp_x', 'passing_Min_x', 'passing_Pos_x'} is not allowed.\n",
      "❌ Error scraping https://fbref.com/en/matches/fb58cf7f/Utah-Royals-Portland-Thorns-FC-April-11-2025-NWSL: Passing 'suffixes' which cause duplicate columns {'passing_Age_x', 'passing_Nation_x', 'passing_#_x', 'passing_Att_x', 'passing_Cmp_x', 'passing_Min_x', 'passing_Pos_x'} is not allowed.\n",
      "❌ Error scraping https://fbref.com/en/matches/71e1c7c8/Seattle-Reign-FC-Portland-Thorns-FC-April-18-2025-NWSL: Passing 'suffixes' which cause duplicate columns {'passing_Age_x', 'passing_Nation_x', 'passing_#_x', 'passing_Att_x', 'passing_Cmp_x', 'passing_Min_x', 'passing_Pos_x'} is not allowed.\n",
      "❌ Error scraping https://fbref.com/en/matches/414d2972/Portland-Thorns-FC-Gotham-FC-April-22-2025-NWSL: Passing 'suffixes' which cause duplicate columns {'passing_Age_x', 'passing_Nation_x', 'passing_#_x', 'passing_Att_x', 'passing_Cmp_x', 'passing_Min_x', 'passing_Pos_x'} is not allowed.\n",
      "❌ Error scraping https://fbref.com/en/matches/1c087799/Portland-Thorns-FC-Racing-Louisville-April-27-2025-NWSL: Passing 'suffixes' which cause duplicate columns {'passing_Age_x', 'passing_Nation_x', 'passing_#_x', 'passing_Att_x', 'passing_Cmp_x', 'passing_Min_x', 'passing_Pos_x'} is not allowed.\n"
     ]
    }
   ],
   "source": [
    "for url in match_urls:\n",
    "    try:\n",
    "        scrape_and_save_full_match_all_tables(url)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f41f78c5-a09b-4922-99ae-58a0cd5b0154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1c087799_summary.csv\n",
      "✅ Saved 1c087799_passing.csv\n",
      "✅ Saved 1c087799_passing_types.csv\n",
      "✅ Saved 1c087799_defense.csv\n",
      "✅ Saved 1c087799_possession.csv\n",
      "✅ Saved 1c087799_misc.csv\n",
      "✅ Saved 1c087799_goalkeepers.csv\n",
      "✅ Saved 1c087799_shots.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def scrape_and_save_full_match_by_table(url):\n",
    "    \"\"\"Scrapes each player stat table separately and saves a CSV for each one.\"\"\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    game_id = url.split(\"/\")[5]\n",
    "    match_date = soup.select_one(\".venuetime\").text.strip() if soup.select_one(\".venuetime\") else \"N/A\"\n",
    "\n",
    "    scorebox = soup.select_one('.scorebox')\n",
    "    teams = []\n",
    "    if scorebox:\n",
    "        team_divs = scorebox.find_all('div', recursive=False)[:2]\n",
    "        for div in team_divs:\n",
    "            team_tag = div.find('a')\n",
    "            if team_tag:\n",
    "                team_name = team_tag.get('title') or team_tag.text\n",
    "                teams.append(team_name.strip())\n",
    "    if len(teams) != 2:\n",
    "        raise Exception(\"Could not find two team names.\")\n",
    "    home_team, away_team = teams\n",
    "\n",
    "    table_types = {\n",
    "        \"summary\": \"Summary\",\n",
    "        \"passing\": \"Passing\",\n",
    "        \"passing_types\": \"Pass Types\",\n",
    "        \"defense\": \"Defensive\",\n",
    "        \"possession\": \"Possession\",\n",
    "        \"misc\": \"Miscellaneous\"\n",
    "    }\n",
    "\n",
    "    join_cols = [\"Player\", \"match_date\", \"game_id\", \"team\", \"opponent\", \"home_away\"]\n",
    "\n",
    "    def flatten_headers(table):\n",
    "        thead = table.find(\"thead\")\n",
    "        rows = thead.find_all(\"tr\")\n",
    "        if len(rows) == 1:\n",
    "            return [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
    "        row1 = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
    "        row2 = [th.text.strip() for th in rows[1].find_all(\"th\")]\n",
    "        flat_headers = []\n",
    "        last_label = \"\"\n",
    "        for i in range(len(row2)):\n",
    "            top = row1[i] if i < len(row1) and row1[i] != \"\" else last_label\n",
    "            last_label = top\n",
    "            bottom = row2[i] if i < len(row2) else \"\"\n",
    "            if top == \"\":\n",
    "                flat_headers.append(bottom)\n",
    "            elif bottom == \"\":\n",
    "                flat_headers.append(top)\n",
    "            else:\n",
    "                flat_headers.append(f\"{top}_{bottom}\")\n",
    "        return flat_headers\n",
    "\n",
    "    for key in table_types:\n",
    "        tables = [t for t in soup.find_all(\"table\") if key in t.get(\"id\", \"\") and \"keeper\" not in t.get(\"id\", \"\")]\n",
    "        tables = tables[:2]  # Limit to 1 per team\n",
    "\n",
    "        all_rows = []\n",
    "        for idx, table in enumerate(tables):\n",
    "            team = home_team if idx == 0 else away_team\n",
    "            opponent = away_team if idx == 0 else home_team\n",
    "            home_away = \"Home\" if idx == 0 else \"Away\"\n",
    "\n",
    "            headers = flatten_headers(table)\n",
    "\n",
    "            for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "                if row.get('class') and \"thead\" in row.get('class'):\n",
    "                    continue\n",
    "                cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "                if len(cells) != len(headers):\n",
    "                    continue\n",
    "                player_data = dict(zip(headers, cells))\n",
    "                if player_data.get(\"Player\", \"\").lower() in [\"team totals\", \"\"]:\n",
    "                    continue\n",
    "                player_data.update({\n",
    "                    \"game_id\": game_id,\n",
    "                    \"match_date\": match_date,\n",
    "                    \"team\": team,\n",
    "                    \"opponent\": opponent,\n",
    "                    \"home_away\": home_away\n",
    "                })\n",
    "                all_rows.append(player_data)\n",
    "\n",
    "        if all_rows:\n",
    "            df = pd.DataFrame(all_rows)\n",
    "            df.to_csv(f\"{game_id}_{key}.csv\", index=False)\n",
    "            print(f\"✅ Saved {game_id}_{key}.csv\")\n",
    "\n",
    "    # GOALKEEPER TABLES\n",
    "    keeper_tables = [t for t in soup.find_all(\"table\") if \"keeper\" in t.get(\"id\", \"\")]\n",
    "    keeper_rows = []\n",
    "\n",
    "    for idx, table in enumerate(keeper_tables):\n",
    "        team = home_team if idx == 0 else away_team\n",
    "        opponent = away_team if idx == 0 else home_team\n",
    "        home_away = \"Home\" if idx == 0 else \"Away\"\n",
    "\n",
    "        headers = flatten_headers(table)\n",
    "\n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            if row.get('class') and \"thead\" in row.get('class'):\n",
    "                continue\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                player_data = dict(zip(headers, cells))\n",
    "                player_data.update({\n",
    "                    \"game_id\": game_id,\n",
    "                    \"match_date\": match_date,\n",
    "                    \"team\": team,\n",
    "                    \"opponent\": opponent,\n",
    "                    \"home_away\": home_away\n",
    "                })\n",
    "                keeper_rows.append(player_data)\n",
    "\n",
    "    if keeper_rows:\n",
    "        df_gk = pd.DataFrame(keeper_rows)\n",
    "        df_gk.to_csv(f\"{game_id}_goalkeepers.csv\", index=False)\n",
    "        print(f\"✅ Saved {game_id}_goalkeepers.csv\")\n",
    "\n",
    "    # SHOTS TABLE\n",
    "    shot_tables = [t for t in soup.find_all(\"table\") if \"shots\" in t.get(\"id\", \"\")]\n",
    "    shots_rows = []\n",
    "\n",
    "    for table in shot_tables:\n",
    "        headers = flatten_headers(table)\n",
    "        for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "            cells = [cell.text.strip() for cell in row.find_all([\"th\", \"td\"])]\n",
    "            if len(cells) == len(headers):\n",
    "                shot_data = dict(zip(headers, cells))\n",
    "                shot_data.update({\n",
    "                    \"game_id\": game_id,\n",
    "                    \"match_date\": match_date\n",
    "                })\n",
    "                shots_rows.append(shot_data)\n",
    "\n",
    "    if shots_rows:\n",
    "        df_shots = pd.DataFrame(shots_rows)\n",
    "        df_shots.to_csv(f\"{game_id}_shots.csv\", index=False)\n",
    "        print(f\"✅ Saved {game_id}_shots.csv\")\n",
    "scrape_and_save_full_match_by_table(\"https://fbref.com/en/matches/1c087799/Portland-Thorns-FC-Racing-Louisville-April-27-2025-NWSL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc6858a-3326-40de-b696-994e69e47bfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### META DATA SCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3eb93654-9d56-4162-b1fd-1da5a8914ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_match_metadata(url):\n",
    "    \"\"\"Scrape metadata from a single match report page.\"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    game_id = url.split(\"/\")[5]\n",
    "\n",
    "    date_element = soup.select_one(\".venuetime\")\n",
    "    match_date = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "    scorebox = soup.select_one('.scorebox')\n",
    "    if not scorebox:\n",
    "        raise Exception(\"Scorebox not found.\")\n",
    "\n",
    "    team_divs = scorebox.find_all('div', recursive=False)[:2]\n",
    "    if len(team_divs) != 2:\n",
    "        raise Exception(\"Expected 2 team divs.\")\n",
    "\n",
    "    teams = []\n",
    "    scores = []\n",
    "    for div in team_divs:\n",
    "        team_tag = div.find('a')\n",
    "        score_tag = div.find('div', class_='score')\n",
    "        if team_tag and score_tag:\n",
    "            team_name = team_tag.get('title') or team_tag.text\n",
    "            teams.append(team_name.strip())\n",
    "            scores.append(score_tag.text.strip())\n",
    "\n",
    "    if len(teams) != 2 or len(scores) != 2:\n",
    "        raise Exception(\"Could not extract both teams and scores.\")\n",
    "\n",
    "    return {\n",
    "        \"game_id\": game_id,\n",
    "        \"match_date\": match_date,\n",
    "        \"home_team\": teams[0],\n",
    "        \"away_team\": teams[1],\n",
    "        \"home_score\": int(scores[0]),\n",
    "        \"away_score\": int(scores[1])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6333951e-c198-49ce-991a-4d4d47ae8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_match_metadata(match_urls):\n",
    "    all_matches = []\n",
    "\n",
    "    for url in match_urls:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        game_id = url.split(\"/\")[-2]\n",
    "\n",
    "        # Match date\n",
    "        date_tag = soup.select_one(\".venuetime\")\n",
    "        match_date = date_tag.text.strip() if date_tag else None\n",
    "\n",
    "        # Teams\n",
    "        scorebox = soup.select_one(\".scorebox\")\n",
    "        teams = scorebox.find_all(\"a\", href=True) if scorebox else []\n",
    "        team_names = [t.get('title') or t.text.strip() for t in teams if \"/en/squads/\" in t['href']]\n",
    "        if len(team_names) != 2:\n",
    "            print(f\"⚠️ Could not determine both teams for {url}\")\n",
    "            continue\n",
    "        home_team, away_team = team_names\n",
    "\n",
    "        # Search for commented-out HTML that contains team stats\n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "        team_stats_table = None\n",
    "        for comment in comments:\n",
    "            if \"team_stats\" in comment:\n",
    "                comment_soup = BeautifulSoup(comment, 'html.parser')\n",
    "                table = comment_soup.find(\"table\", id=\"team_stats\")\n",
    "                if table:\n",
    "                    team_stats_table = table\n",
    "                    break\n",
    "\n",
    "        if not team_stats_table:\n",
    "            print(f\"⚠️ No team stats table found for {url}\")\n",
    "            continue\n",
    "\n",
    "        # Extract stat rows\n",
    "        stats = {\n",
    "            \"game_id\": game_id,\n",
    "            \"match_date\": match_date,\n",
    "            \"home_team\": home_team,\n",
    "            \"away_team\": away_team\n",
    "        }\n",
    "\n",
    "        rows = team_stats_table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) == 3:\n",
    "                label = cells[0].text.strip().lower().replace(\" \", \"_\")\n",
    "                stats[f\"home_{label}\"] = cells[1].text.strip()\n",
    "                stats[f\"away_{label}\"] = cells[2].text.strip()\n",
    "\n",
    "        all_matches.append(stats)\n",
    "\n",
    "    if all_matches:\n",
    "        df = pd.DataFrame(all_matches)\n",
    "        df.to_csv(\"match_data.csv\", index=False)\n",
    "        print(\"✅ Saved match_data.csv\")\n",
    "    else:\n",
    "        print(\"⚠️ No match data collected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "72759dda-8eeb-4668-a71f-cf0d6b6c0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No team stats table found for https://fbref.com/en/matches/13e27d90/Portland-Thorns-FC-Orlando-Pride-May-3-2025-NWSL\n",
      "⚠️ No match data collected\n"
     ]
    }
   ],
   "source": [
    "match_urls = [\n",
    "    # \"https://fbref.com/en/matches/7239a666/Kansas-City-Current-Portland-Thorns-FC-March-15-2025-NWSL\",\n",
    "    # \"https://fbref.com/en/matches/7958f078/Portland-Thorns-FC-Angel-City-FC-March-21-2025-NWSL\",\n",
    "    # \"https://fbref.com/en/matches/475a847a/Portland-Thorns-FC-North-Carolina-Courage-March-29-2025-NWSL\",\n",
    "    # \"https://fbref.com/en/matches/fb58cf7f/Utah-Royals-Portland-Thorns-FC-April-11-2025-NWSL\",\n",
    "    # \"https://fbref.com/en/matches/71e1c7c8/Seattle-Reign-FC-Portland-Thorns-FC-April-18-2025-NWSL\",\n",
    "    # \"https://fbref.com/en/matches/414d2972/Portland-Thorns-FC-Gotham-FC-April-22-2025-NWSL\",\n",
    "    # \"https://fbref.com/en/matches/1c087799/Portland-Thorns-FC-Racing-Louisville-April-27-2025-NWSL\",\n",
    "    \"https://fbref.com/en/matches/13e27d90/Portland-Thorns-FC-Orlando-Pride-May-3-2025-NWSL\"\n",
    "    # Add all your match report URLs here\n",
    "]\n",
    "\n",
    "scrape_match_metadata(match_urls)\n",
    "\n",
    "# # Collect metadata\n",
    "# match_metadata = []\n",
    "\n",
    "# for url in match_urls:\n",
    "#     try:\n",
    "#         data = scrape_match_metadata(url)\n",
    "#         match_metadata.append(data)\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed to scrape {url}: {e}\")\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_metadata = pd.DataFrame(match_metadata)\n",
    "\n",
    "# # Save to CSV\n",
    "# df_metadata.to_csv(\"thorns_2025_match_metadata.csv\", index=False)\n",
    "# print(\"✅ Saved match metadata to thorns_2025_match_metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad39c9-68ed-418c-afcc-ffa3abb03d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
